# MDP에서의 보상함수 및 상태 전이 매트릭스
[[마르코프 결정 과정(MDP)]]에서 에이전트의 **행동**은 오로지 **정책**에 의해 결정되며 정책은 시간에 따라 변하지 않는다. 또한 MDP는 마르코프 속성을 가정하기 때문에 정책은 과거의 상태가 아닌 오로지 현재의 상태에만 영향을 받는다.

>$P^\pi_{ss'} = \sum_{a \in A}\pi(a | s)P^a_{ss'}$ 
>$R^\pi_{s} = \sum_{a \in A}\pi(a | s)R^a_s$
