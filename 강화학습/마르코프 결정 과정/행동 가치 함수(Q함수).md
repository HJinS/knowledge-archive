가치를 극대화하는 [[정책]]이란 정책을 따라 행동했을 떄 가치 함수의 결과가 가장 좋게 나와야 한다.
즉, 이를 위해서는 행도에 따른 가치를 평가하는 함수가 필요하다. 이를 행동 가치 함수(**Q: Action Value Function**)라고 하며, Q함수 라고 부른다.

> **Q함수**란 선택할 수 있는 여러 가지 행동 중에 하나를 선택했을 때의 가치를 계산하는 함수이다.

$v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s]$
$v_\pi(s) = \sum_{a \in A}\pi(a | s)R_s^a + \gamma \sum_{a \in A}\pi(a | s)$ - 1


$q(s,a) = E_\pi[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}|S_t = s,A_t = a)]$ - 2
$q(s,a) = R_s^a + \gamma \sum_{s' \in S}P_{ss'}^a\pi(s',a')q_\pi(s',a')$ - 3

> 행동 가치 함수(Q함수)는 선택할 수 있는 여러 가지 행동 중에 하나를 선택했을 때의 가치를 계산 하는 것이다.

1. [[상태 가치 함수(MDP)]]
2. 행동 가치 함수에서는 행동을 미리 선택하였기 때문에, 행동에 대한 부분을 뺴고 3번처럼 만들 수 있다.
3. 다음 상태에서의 보상을 정확히 계산하기 위해서는 **행동을 선택하는 확률 매트리스(정책, $\pi$)** 와 상태 **전이 확률 매트릭스(P)** 를 곱해주어야 한다.

> MDP에서 하나의 상태에서 다른 상태로 이동하기 위해서는 [[상태 전이 매트릭스]]와 함께 행동을 선택할 확률([[정책]], $\pi$)
> -> 행동 가티 함수를 사용해서 상태 가치 함수를 구하기 위해서는 정책에 대한 **기대값**즉, 평균을 구해야 한다.
