# 마르코프 결정 과정(Markov Decision Process)
**마르코프 결정 과정**은 [[마르코프 보상 과정(MRP)]]에 행동(Action)과 정책(Policy)이 추가된 개념이다.
> [[마르코프 결정 과정(MDP)]]가 에피소드나 [[환경]] 전체의 가치를 계산하는 것이 목적이라면 **MDP**는 환경의 가치를 **극대화**하는 정책을 결정하는 것이 목적이다.

|MRP|에이전트는 시간의 흐름(타임스텝)에 따라 상태 전이 확률에 영향을 받으며 자연스럽게 이동|
|---|---|
|**MDP**|**에이전트는 타임스텝별로 정책에 따라 행동을 선택하고 상태 전이 확률에 영향을 받아 이동**|

---
## MDP 구성 요소

>$S$: 상태(State)의 집합
>$P$: 상태 전이 매트릭스
>$P_{SS'} = P[S_{t+1} = s' | S_t = s]$
>$R$: 보상 함수
>$\gamma$: 감가율($\gamma$ $\in$ [0, 1])
>$A$: 행동(Action)의 집합
>$\pi$: [[정책]] 함수

MDP에서는 **행동**이 추가되었기 때문에 상태 전이 매트릭스와 [[보상 함수]]또한 행동을 함께 생각해줘야 한다. MDP에서 취할 수 있는 행동의 개수는 상태와 마찬가지로 유한하다.
![[MRP MDP 비교|1000|]]
> t에서 s1일 때 t2에서 s2일 확률
> MRP = p1 = 0.7
> MDP = $\pi*p1 + \pi2*p1 = 0.4*0.7 + 0.6*0.7$

**MDP**환경에서 s2에 있을 확률
- 에이전트가 정책에 따라 a1을 선택했다고 해도 반드시 s2로 이동하지는 않는다. **상태 전이 확률**의 영향을 받기 때문이다. 정책에 대한 확률과 상태 전이 확률을 곱해서 더해야 한다.
