# **마르코프 보상 과정**(Markov Reward Process)
[[마르코프 연쇄]]에 보상과 시간에 따른 보상의 감가율을 의미하는 $\gamma$ 가 추가된 것이다.

> $S$: 상태(State)의 집합
> $P$: 상태 전이 매트릭스
> 	$P_{SS'} = P[S_{t+1} = s' | S_t = s]$ 
> $R$: 보상 함수
> $\gamma$: 감가율($\gamma$ $\in$ [0, 1])

### S(Set of Sates)
다루고 있는 **[[환경]]**이 가질 수 있는 다양한 상태이다. **MRP**에서 상태는 유한해야 한다.
### P(State Transition Matrix)
각각의 상태가 다른 상태로 변할 수 있는 **조건부 확률**을 매트릭스 형태로 표현 한 것이다.
> 시간 t에서 상태가 s일때, 시간 t+1에서 상태가 s'이 될 조건부 확률

### R(Reward Function)
보상함수는 확률의 기대값(E: Expectation)형태로 표현 할 수 있다.
> 시간 **t에서 상태가 s일 때, 시간 t+1**에서 받을 수 있는 보상의 기대값이다.
$R_{s1} = P1 * r1 + p2 * r2$

![[보상함수 샘플]]
시간 t에서 상태가 s1일 때 보상함수를 통해서 구할 수 있는 보상은 그 순간에 받는 보상만을 계산 할 수 있다. -> s1의 보상이 게산되는 시점은 t+1이다.
시간이 한 타임스텝 지나서 t+1로 이동되고 상태는 s'으로 이동될 때, 상태 s의 보상이 계산되는 것이다.

### $\gamma$
**감가율(할인율)**을 의미한다. 0, 1사이의 값을 가질 수 있다.
일반적으로 시간의 흐름에 따라 가치를 얼마의 비율로 할인할지를 결정하는 비율이다.
> MRP의 목적은 하나의 에피소드 혹은 전체 환경의 가치를 계산하는 것이다.
> 그리고 그 가치는 현재의 가치로 환산되어야 하기에 감가율이 필요하다.

## 반환값(G: Return)
타임스텝 t에서 계산한 누적 보상의 합계이다. 이 때, 누적 보상은 감가율로 할인되어 계산된다.
주로 에피소드 단위로 계산이 되며, 에피소드의 효율성이나 가치를 판환값을 통해 평가되며, 이 반환값을 **극대화** 하는 환경을 설계하는 것이 MRP의 목적이다.
> $G_t = R_{t+1} + R_{t+2} + ... = \sum_{k=0}^{\inf|\gamma^k $

