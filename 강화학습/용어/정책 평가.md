[[마르코프 결정 과정(MDP)]]에서 [[정책]]을 평가하는 것은 [[상태 가치 함수(MDP)]]를 구하는 것과 같다.

> 상태 가치 함수란 **어떤 정책($\pi$: *Policy*)을 따랐을 때 보상의 총 합계를 구하는 것**이다. 따라서 *상태 가치 함수*를 통해 확인 할 수 있는 보상이 정책의 효율성을 말한다.